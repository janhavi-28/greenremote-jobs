"""
normalizer.py – Clean and normalise raw LinkedIn job data so it matches
the `public.jobs` Supabase schema exactly.

Schema target
─────────────
  id              uuid        (generated by Supabase)
  title           text
  company         text
  location        text
  description     text
  employment_type text
  source          text
  source_url      text  UNIQUE
  created_at      timestamp   (default now())
"""

from __future__ import annotations

import re
import uuid
from typing import Any, Dict, Optional
from urllib.parse import urlparse, urlunparse, parse_qs, urlencode

from bs4 import BeautifulSoup

# ── HTML cleaning ─────────────────────────────────────────────────────────────

_MULTI_SPACE = re.compile(r"[ \t]+")
_MULTI_NEWLINE = re.compile(r"\n{3,}")


def _strip_html(raw: str | None) -> str:
    """Remove HTML tags and decode entities; normalise whitespace."""
    if not raw:
        return ""
    soup = BeautifulSoup(raw, "lxml")
    text = soup.get_text(separator="\n")
    text = _MULTI_SPACE.sub(" ", text)
    text = _MULTI_NEWLINE.sub("\n\n", text)
    return text.strip()


# ── URL normalisation ─────────────────────────────────────────────────────────

_LI_TRACKING_PARAMS = {"trackingId", "refId", "trk", "trkInfo", "position", "pageNum"}


def _clean_url(raw_url: str | None) -> str:
    """
    Normalise a LinkedIn job URL to a canonical form suitable for upsert.

    Strips tracking query parameters so the same job doesn't get inserted
    twice just because LinkedIn added a different tracking token.
    """
    if not raw_url:
        return ""
    try:
        parsed = urlparse(raw_url.strip())
        qs = parse_qs(parsed.query, keep_blank_values=False)
        clean_qs = {k: v for k, v in qs.items() if k not in _LI_TRACKING_PARAMS}
        clean_parsed = parsed._replace(query=urlencode(clean_qs, doseq=True))
        return urlunparse(clean_parsed)
    except Exception:
        return raw_url.strip()


# ── Field helpers ─────────────────────────────────────────────────────────────

def _clean_text(value: Any, max_len: int = 512) -> str:
    """Strip, truncate, and return a safe string."""
    if not isinstance(value, str):
        value = str(value) if value is not None else ""
    cleaned = value.strip()
    if len(cleaned) > max_len:
        cleaned = cleaned[:max_len]
    return cleaned


_EMPLOYMENT_ALIASES: Dict[str, str] = {
    "full time": "Full-Time",
    "full-time": "Full-Time",
    "fulltime": "Full-Time",
    "part time": "Part-Time",
    "part-time": "Part-Time",
    "parttime": "Part-Time",
    "contract": "Contract",
    "contractor": "Contract",
    "freelance": "Freelance",
    "internship": "Internship",
    "intern": "Internship",
    "temporary": "Temporary",
    "temp": "Temporary",
    "volunteer": "Volunteer",
    "other": "Other",
}


def _normalise_employment_type(raw: str | None) -> str:
    if not raw:
        return ""
    key = raw.strip().lower()
    return _EMPLOYMENT_ALIASES.get(key, raw.strip().title())


# ── Main normalise function ───────────────────────────────────────────────────

def normalise_job(raw: Dict[str, Any], source: str = "LinkedIn") -> Optional[Dict[str, Any]]:
    """
    Convert a raw scraped dict into a row ready for `public.jobs`.

    Returns None if the job is missing a valid source_url (can't upsert without it).
    """
    source_url = _clean_url(raw.get("source_url") or raw.get("url") or raw.get("link"))
    if not source_url:
        return None

    title       = _clean_text(raw.get("title"), max_len=512)
    company     = _clean_text(raw.get("company"), max_len=256)
    location    = _clean_text(raw.get("location") or raw.get("place"), max_len=256)
    description = _strip_html(raw.get("description") or raw.get("desc"))
    emp_type    = _normalise_employment_type(raw.get("employment_type") or raw.get("type"))

    return {
        "title":           title,
        "company":         company,
        "location":        location,
        "description":     description,
        "employment_type": emp_type,
        "source":          source,
        "source_url":      source_url,
    }
