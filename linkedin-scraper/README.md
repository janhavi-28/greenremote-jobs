# LinkedIn Scraper Service

A production-ready Python service that **scrapes LinkedIn job listings and upserts them into the `public.jobs` Supabase table** used by the GreenRemote Jobs Next.js frontend.

This service is fully isolated — it **never touches the Next.js codebase** and runs as a completely independent background process.

---

## Folder Structure

```
linkedin-scraper/
├── .env.example        ← Template — copy to .env and fill in values
├── .env                ← Your real secrets (git-ignored)
├── .gitignore
├── requirements.txt    ← Python dependencies
├── Dockerfile          ← For Railway / Docker deployments
│
├── config.py           ← Centralised configuration (reads .env)
├── logger.py           ← Coloured logging setup
├── scraper.py          ← Playwright browser automation (LinkedIn)
├── normalizer.py       ← HTML cleaning, URL deduplication, field mapping
├── translator.py       ← Safe language detection + English translation
├── db.py               ← Supabase upsert layer (service-role key)
├── pipeline.py         ← Orchestrates scrape → normalise → translate → upsert
└── main.py             ← Entry point with CLI flags and scheduler
```

---

## Prerequisites

| Requirement | Min version |
|---|---|
| Python | 3.10+ |
| pip | any recent |
| Playwright Chromium | installed via `playwright install` |

---

## Setup

### 1. Create and activate a virtual environment

```bash
cd linkedin-scraper
python -m venv .venv

# Windows
.venv\Scripts\activate

# macOS / Linux
source .venv/bin/activate
```

### 2. Install Python dependencies

```bash
pip install -r requirements.txt
```

### 3. Install Playwright browser

```bash
python -m playwright install --with-deps chromium
```

### 4. Configure environment variables

```bash
# The .env is already pre-filled with your Supabase project credentials.
# Just add optional LinkedIn credentials if you want session-based scraping:
```

Edit `.env`:
```env
SUPABASE_URL=https://your-project-ref.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key-here

# Optional — enables richer job data extraction
LINKEDIN_EMAIL=your-email@example.com
LINKEDIN_PASSWORD=your-password
```

---

## Running the Service

### Smoke test (validate config + DB connection)
```bash
python main.py --test
```

### Single run (scrape once and exit)
```bash
python main.py --once
```

### Scheduled mode (runs every 6 hours, forever)
```bash
python main.py
```

---

## Database Schema (existing `public.jobs` table)

| Column | Type | Notes |
|---|---|---|
| `id` | uuid | Primary key, generated by Supabase |
| `title` | text | Job title |
| `company` | text | Company name |
| `location` | text | Job location |
| `description` | text | Full job description (HTML stripped) |
| `employment_type` | text | e.g. Full-Time, Contract |
| `source` | text | Always `"LinkedIn"` |
| `source_url` | text UNIQUE | Canonical job URL — upsert conflict key |
| `created_at` | timestamp | Defaults to `now()` |

### Upsert logic
- **Same `source_url`** → row is **updated** (latest data wins)
- **New `source_url`** → row is **inserted**
- Both counts are logged at the end of each pipeline run

---

## Architecture Notes

### Translation safety
`translator.py` runs `langdetect` before calling the Google Translate API. This prevents the infamous `"AUTO IS INVALID SOURCE LANGUAGE"` error:
- If language detection fails → original text is kept
- If detected language is `en` → text is returned unchanged (fast path)
- If translated result contains error strings → original text is kept
- All translation API calls are retried up to 3× with exponential back-off

### URL deduplication
`normalizer.py` strips LinkedIn's tracking query parameters (e.g. `trackingId`, `refId`, `trk`) from job URLs before storing them. This ensures the same job posted with different tracking tokens is still recognised as a duplicate.

### Rate limiting
- Random delay between `DELAY_MIN` and `DELAY_MAX` seconds between every page action
- Human-like scroll behaviour on search result pages
- Chromium launched with `--disable-blink-features=AutomationControlled`

---

## Deployment

### Option A – GitHub Actions (recommended, free)

1. Push the repo to GitHub
2. Add secrets in **Settings → Secrets → Actions**:
   - `SUPABASE_URL`
   - `SUPABASE_SERVICE_ROLE_KEY`
   - `LINKEDIN_EMAIL` *(optional)*
   - `LINKEDIN_PASSWORD` *(optional)*
3. The workflow at `.github/workflows/linkedin-scraper.yml` runs automatically every 6 hours and can be triggered manually from the Actions tab.

### Option B – Railway (always-on container)

1. Create a new Railway project
2. Connect your GitHub repo, select the `linkedin-scraper` directory
3. Railway will detect the `Dockerfile` automatically
4. Add environment variables in the Railway Variables panel
5. The container runs `python main.py` (scheduler mode) and stays alive

### Option C – Local cron (PowerShell Task Scheduler / crontab)

```bash
# Run once every 6 hours
0 */6 * * * cd /path/to/greenremote-jobs/linkedin-scraper && .venv/bin/python main.py --once >> logs/scraper.log 2>&1
```

---

## Customisation

| Variable | Default | Description |
|---|---|---|
| `SEARCH_QUERIES` | `remote developer,...` | Comma-separated LinkedIn search terms |
| `SEARCH_LOCATIONS` | `Worldwide` | Comma-separated LinkedIn locations |
| `MAX_JOBS_PER_RUN` | `150` | Hard cap on jobs per execution |
| `DELAY_MIN` | `2` | Seconds (min delay between actions) |
| `DELAY_MAX` | `5` | Seconds (max delay between actions) |
| `HEADLESS` | `true` | Set to `false` to watch the browser |

---

## Security

- The **service-role key** is never exposed to the frontend or committed to git
- `.env` is listed in both root `.gitignore` and `linkedin-scraper/.gitignore`
- The scraper uses Supabase's service-role key which bypasses RLS — handle carefully
