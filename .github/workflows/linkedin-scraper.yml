# ──────────────────────────────────────────────────────────────────────────────
# GitHub Actions workflow — runs the scraper every 6 hours
#
# Secrets required (add in repo Settings → Secrets → Actions):
#   SUPABASE_URL
#   SUPABASE_SERVICE_ROLE_KEY
#   LINKEDIN_EMAIL        (optional)
#   LINKEDIN_PASSWORD     (optional)
# ──────────────────────────────────────────────────────────────────────────────

name: LinkedIn Scraper

on:
  schedule:
    - cron: "0 */6 * * *"   # Every 6 hours
  workflow_dispatch:         # Allow manual trigger from Actions UI

jobs:
  scrape:
    name: Scrape LinkedIn Jobs
    runs-on: ubuntu-latest
    timeout-minutes: 30

    defaults:
      run:
        working-directory: linkedin-scraper

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip
          cache-dependency-path: linkedin-scraper/requirements.txt

      - name: Install Python dependencies
        run: pip install -r requirements.txt

      - name: Install Playwright browsers
        run: python -m playwright install --with-deps chromium

      - name: Run scraper (single run)
        env:
          SUPABASE_URL:              ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          LINKEDIN_EMAIL:            ${{ secrets.LINKEDIN_EMAIL }}
          LINKEDIN_PASSWORD:         ${{ secrets.LINKEDIN_PASSWORD }}
          HEADLESS:                  "true"
          MAX_JOBS_PER_RUN:          "150"
        run: python main.py --once
